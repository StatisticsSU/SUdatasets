dim(rainfall)
plot(rainfall)
plot(rainfall$precipitation)
# Make datafile in package
use_data(rainfall)
devtools::document()
?rainfall
# load the data from file
load("/home/mv/Dropbox/devR/BayesLearnSU/data/womenWork.RData")
womenwork = womenwork
womenwork = womenWork
# Make datafile in package
use_data(womenwork)
# Add documentation and then do
devtools::install()
# Add documentation and then do
devtools::install()
devtools::document()
exp(3.5)
log(33)
devtools::install()
devtools::document()
?SUdatasets
?tempLinkoping
help(, "SUdatasets")
?SUdatasets
library(SUdatasets)
?SUdatasets
?bike
help(biks)
help(bike)
help(, "SUdatasets")
devtools::install()
library(SUdatasets)
remove.packages("SUdatasets")
devtools::install()
library(SUdatasets)
?SUdatasets
help(, "SUdatasets")
devtools::install()
library(SUdatasets)
help(, "SUdatasets")
help(, "SUdatasets")
devtools::install()
devtools::install()
library(SUdatasets)
help(, "SUdatasets")
# load the data from file
load("/home/mv/Dropbox/BayesBook/Data/RajanData.csv")
# load the data from file
read.csv("/home/mv/Dropbox/BayesBook/Data/RajanData.csv", header = T)
# load the data from file
debtratio = read.csv("/home/mv/Dropbox/BayesBook/Data/RajanData.csv", header = T)
# Make datafile in package
use_data(debtratio)
# Add documentation and then do
devtools::install()
debtratio
head(debtratio)
# Add documentation and then do
devtools::install()
devtools::document()
?debtratio
#-------------------------------------------------------------------------------
# lm_diagnostics():
#-------------------------------------------------------------------------------
library(ggplot2)
library(cowplot)
install.packages("cowplot")
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
library(regkurs)
?regsummary
lmfit = lm(nRides ~ temp + hum + windspeed, data = bike)
lm_diagnostics(lmfit)
model_diagnostics(lmfit$residuals,lmfit$fitted.values)
model_diagnostics(lmfit$residuals,lmfit$fitted.values, binwidth = 10)
model_diagnostics(lmfit$residuals,lmfit$fitted.values, binwidth = 10)
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
?stat_qq
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
?stat_qq
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
source("~/Downloads/lm_diagnostics.R", echo=TRUE)
lm_diagnostics(lmfit)
library(remotes)
install_github("/StatisticsSU/regkurs")
install_github("StatisticsSU/regkurs")
library(regkurs)
?res.diagnostics
library(regkurs)
?res.diagnostics
lmfit = lm(nRides ~ temp + hum + windspeed, data = bike)
res.diagnostics(lmfit)
res.diagnostics(lmfit,block=T)
res.diagnostics(lmfit,outliers=T)
res.diagnostics(lmfit,outliers=T)
res.diagnostics(lmfit,block=T)
res.diagnostics(lmfit,block=T)
res.diagnostics(lmfit,outliers=T)
res.diagnostics(lmfit,outliers=T)
res.diagnostics(lmfit)
res.diagnostics(lmfit)
res.diagnostics(lmfit,outliers=T)
res.diagnostics(lmfit,block=T)
res.diagnostics(lmfit,outliers=T)
res.diagnostics(lmfit,outliers=T)
res.diagnostics(lmfit,block=T)
res.diagnostics(lmfit,block=T)
res.diagnostics(lmfit,outliers=T)
res.diagnostics(lmfit,block=T)
res.diagnostics(lmfit,block=T)
res.diagnostics(lmfit,block=T)
res.diagnostics(lmfit,block=T)
res.diagnostics(lmfit,block=T)
res.diagnostics(lmfit,block=T);
res.diagnostics(lmfit,block=T);
library(devtools)
library(roxygen2)
# load the data from file
globaltemp = read.csv("/home/mv/Dropbox/Teaching/Regression/Data/globaltemp.csv", header = T)
# Make datafile in package
use_data(globaltemp)
globaltemp
dim(globaltemp)
is.data.frame(globaltemp)
# load the data from file
globaltemp = read.csv("/home/mv/Dropbox/Teaching/Regression/Data/globaltemp.csv", header = T)
names(globaltemp) <- c("year", "temp", "smoothtemp")
head(globaltemp)
# load the data from file
globaltemp = read.csv("/home/mv/Dropbox/Teaching/Regression/Data/globaltemp.csv", header = T)
names(globaltemp) <- c("year", "temp", "smoothtemp")
# Make datafile in package
use_data(globaltemp)
# Make datafile in package
use_data(globaltemp, overwrite = T)
# Add documentation and then do
devtools::install()
devtools::document()
# load the data from file
globaltemp = read.csv("/home/mv/Dropbox/Teaching/Regression/Data/globaltemp.csv", header = T)
names(globaltemp) <- c("year", "temp", "smoothtemp")
# Make datafile in package
use_data(globaltemp, overwrite = T)
library(devtools)
install.packages("devtools")
install.packages("roxygen2")
library(devtools)
install.packages("devtools")
# Problem 1a)
load("noncentral.RData")
logPost <- function(lambda, x){
logLik = sum(dchisq(x, df = 2, ncp = lambda, log = TRUE))
logPrior = dexp(lambda, rate = 1, log = TRUE)
return(logLik + logPrior)
}
lambdaGrid = seq(0.01, 5, length = 1000)
postDens = rep(0, length(lambdaGrid))
for (i in 1:length(lambdaGrid)){
postDens[i] = exp(logPost(lambdaGrid[i], x))
}
postDens = postDens/(sum(postDens)*(lambdaGrid[2]-lambdaGrid[1])) # normalizing
plot(lambdaGrid, postDens, type = "l", xlab = expression(lambda), ylab = "density")
# Problem 1b)
# Equal tail interval
postcdf = cumsum(postDens*(lambdaGrid[2]-lambdaGrid[1]))
equaltail = c(lambdaGrid[which.min(postcdf<=0.025)], lambdaGrid[which.min(postcdf<=0.975)])
message(paste("The 95% equal tail interval is: ["), round(equaltail[1],3),",",round(equaltail[2],3),"]")
lines(equaltail, c(0,0), col = "red", lwd = 4) # Plotting the interval as a line, just for fun.
# Problem 1a)
load("noncentral.RData")
prettycol = RColorBrewer::brewer.pal(12, "Paired")[c(1,2,7,8,3,4,5,6,9,10,11,12)]
library(mvtnorm)
setwd('/home/mv/Dropbox/Teaching/BayesLearning/Hidden/Exam/20220525/')
# Problem 1a)
load("noncentral.RData")
logPost <- function(lambda, x){
logLik = sum(dchisq(x, df = 2, ncp = lambda, log = TRUE))
logPrior = dexp(lambda, rate = 1, log = TRUE)
return(logLik + logPrior)
}
lambdaGrid = seq(0.01, 5, length = 1000)
postDens = rep(0, length(lambdaGrid))
for (i in 1:length(lambdaGrid)){
postDens[i] = exp(logPost(lambdaGrid[i], x))
}
postDens = postDens/(sum(postDens)*(lambdaGrid[2]-lambdaGrid[1])) # normalizing
plot(lambdaGrid, postDens, type = "l", xlab = expression(lambda), ylab = "density")
# Problem 1b)
# Equal tail interval
postcdf = cumsum(postDens*(lambdaGrid[2]-lambdaGrid[1]))
equaltail = c(lambdaGrid[which.min(postcdf<=0.025)], lambdaGrid[which.min(postcdf<=0.975)])
message(paste("The 95% equal tail interval is: ["), round(equaltail[1],3),",",round(equaltail[2],3),"]")
lines(equaltail, c(0,0), col = "red", lwd = 4) # Plotting the interval as a line, just for fun.
Log_post<-function(x, lambda){
return(sum(dchisq(x, df=2, ncp = lambda, log = TRUE))+dexp(lambda, rate = 1, log = TRUE))
}
lambdaGrid<-seq(0,5, length.out = 1000)
postdens_sample<-c()
for (i in 1:length(lambdaGrid)) {
a<-Log_post(x, lambda = lambdaGrid[i])
postdens_sample<-c(postdens_sample,a)
}
delta<-lambdaGrid[2]-lambdaGrid[1]
postdens<-exp(postdens_sample)/sum(delta*exp(postdens_sample))
plot(lambdaGrid, postdens, type = "l", xlab = "lambda", ylab ="Posterior density", main = "Approx posterior for lambda")
postmode1b<-lambdaGrid[which.max(postdens)]
postdens1b<-cumsum(postdens)/sum(postdens)
postdens1b_low<-which(postdens1b>=0.05)[1]
postdens1b_high<-which(postdens1b>=0.95)[1]
CI<-lambdaGrid[c(postdens1b_low, postdens1b_high)]
plot(lambdaGrid, postdens, type = "l", xlab = "lambda", ylab ="Posterior density", main = "Approx posterior for lambda")
abline(v=postmode1b, col="red")
abline(v=CI[1], col="purple")
abline(v=CI[2], col="purple")
CI
setwd('/home/mv/Dropbox/Teaching/BayesLearning/Hidden/Exam/20220525/')
# Problem 1a)
load("noncentral.RData")
logPost <- function(lambda, x){
logLik = sum(dchisq(x, df = 2, ncp = lambda, log = TRUE))
logPrior = dexp(lambda, rate = 1, log = TRUE)
return(logLik + logPrior)
}
lambdaGrid = seq(0.01, 5, length = 1000)
postDens = rep(0, length(lambdaGrid))
for (i in 1:length(lambdaGrid)){
postDens[i] = exp(logPost(lambdaGrid[i], x))
}
postDens = postDens/(sum(postDens)*(lambdaGrid[2]-lambdaGrid[1])) # normalizing
plot(lambdaGrid, postDens, type = "l", xlab = expression(lambda), ylab = "density")
# Problem 1b)
# Equal tail interval
postcdf = cumsum(postDens*(lambdaGrid[2]-lambdaGrid[1]))
equaltail = c(lambdaGrid[which.min(postcdf<=0.025)], lambdaGrid[which.min(postcdf<=0.975)])
message(paste("The 95% equal tail interval is: ["), round(equaltail[1],3),",",round(equaltail[2],3),"]")
lines(equaltail, c(0,0), col = "red", lwd = 4) # Plotting the interval as a line, just for fun.
CI
# Problem 1c)
# Solution 1: One can recall that lin-lin loss with constants c1 and c2 gives that
# c1/(c1+c2) percentile of posterior is optimal. Here c1 = 1 and c2 = 2, so 1/(1+2) = 1/3 percentile is optimal.
# Check: we have a large loss for overestimation (a>lambda) so it is optimal to choose a lower (33.3%) percentile
# in the posterior to not risk overestimating.
# Solution 2: If you don't remember the result in Solution 1, you can solve this
# by "brute force" by finding a that minimizes posterior
# expected loss using numerical integration of: integral L(a,lambda)p(lambda|x1,...,xn) d lambda
postcdf = cumsum(postDens*(lambdaGrid[2]-lambdaGrid[1]))
est_linlin = lambdaGrid[which.min(postcdf<=1/3)]
message(paste("The optimal estimate is the 33.333% percentile: "), est_linlin)
points(est_linlin, 0, pch = 19, col = "blue")
?dchisq
# Load the data
load("doctorvisit.RData")
# Fitting a plain glm just to get some feel for the data. Not necessary.
data = data.frame(y, X)
names(data) <- c("y", "const", "reform", "badh","age", "educ", "loginc")
glmfit = glm(y ~ 0 + ., family = poisson, data = data)
summary(glmfit)
# Define the log posterior function for negative binomial regression
logPostNegBin <- function(param, y, X){
p = dim(X)[2]
beta_ = param[1:p]
alpha = param[p+1]
psi = exp(alpha)
Xbeta = X%*%beta_
logLik = sum(dnbinom(y, size = psi, mu = exp(Xbeta), log = T))
logPrior = dmvnorm(beta_, mean = rep(0,p), sigma = 1^2*diag(p), log = T) +
dnorm(alpha, mean = 0, sd = 1, log = T)
return(logLik + logPrior)
}
# Define the log posterior function for Poisson regression
logPostPois <- function(param, y, X){
p = dim(X)[2]
beta_ = param
Xbeta = X%*%beta_
logLik = sum(dpois(y, lambda = exp(Xbeta), log = T))
logPrior = dmvnorm(beta_, mean = rep(0,p), sigma = 1^2*diag(p), log = T)
return(logLik + logPrior)
}
## Run optim for negative binomial
p = dim(X)[2] + 1 # Total number of parameters, including the overdispersion
initVal <- as.matrix(rep(0,p))
OptimResults <- optim(initVal, logPostNegBin, gr=NULL, y, X,
method=c("BFGS"), control=list(fnscale=-1), hessian=TRUE)
postMode = OptimResults$par
postCov = -solve(OptimResults$hessian) # inv(J) - Approx posterior covariance matrix
# Laplace approximation
logm_negbin = logPostNegBin(postMode, y, X) + 0.5*log(det(postCov)) + (p/2)*log(2*pi)
## Run optim for poisson
p = dim(X)[2] # Total number of parameters, no overdispersion here
initVal <- as.matrix(rep(0,p))
OptimResults <- optim(initVal, logPostPois, gr=NULL, y, X,
method=c("BFGS"), control=list(fnscale=-1), hessian=TRUE)
postMode = OptimResults$par
postCov = -solve(OptimResults$hessian) # inv(J) - Approx posterior covariance matrix
# Laplace approximation
logm_pois = OptimResults$value + 0.5*log(det(postCov)) + (p/2)*log(2*pi)
p_negbin = exp(logm_negbin)/(exp(logm_negbin) + exp(logm_pois))
p_negbin
message(paste("Posterior probability for neg binomial model: ",round(p_negbin,4)))
# Load the data
load("carsdata.RData")
logm_pois
logm_pois/logm_negbin
# Load the dataload("carsdata.RData")
data = data.frame(y, X)
# Load the data
load("carsdata.RData")
data = data.frame(y, X)
names(data) <- c("mpg", "intercept", "weight", "sixcyl", "eightcyl", "manual")
# Load the data
load("carsdata.RData")
data = data.frame(y, X)
names(data) <- c("mpg", "intercept", "weight", "sixcyl", "eightcyl", "manual")
# Fit using least squares, just to have an idea of what is going on:
lmfit = lm(mpg ~ 0 + . , data = data)
summary(lmfit)
mu_0 = rep(0,5)
Omega_0 = 1*diag(5)
v_0 = 5
sigma2_0 = 5
nIter = 10000
res = BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter)
par(mfrow = c(2,3))
hist(sqrt(res$sigma2Sample), 50, main = "sigma")
hist(res$betaSample[,1], 50, main = expression(beta[0]))
hist(res$betaSample[,2], 50, main = expression(beta[1]))
hist(res$betaSample[,3], 50, main = expression(beta[2]))
hist(res$betaSample[,4], 50, main = expression(beta[3]))
hist(res$betaSample[,5], 50, main = expression(beta[4]))
# Load the data
load("carsdata.RData")
data = data.frame(y, X)
names(data) <- c("mpg", "intercept", "weight", "sixcyl", "eightcyl", "manual")
# Fit using least squares, just to have an idea of what is going on:
lmfit = lm(mpg ~ 0 + . , data = data)
summary(lmfit)
mu_0 = rep(0,5)
Omega_0 = 1*diag(5)
v_0 = 5
sigma2_0 = 5
nIter = 10000
source("~/Dropbox/Teaching/BayesLearning/Hidden/Exam/20220525/BLExam220525.R", echo=TRUE)
source("~/Dropbox/Teaching/BayesLearning/Hidden/Exam/20220525/BLExam220525.R", echo=TRUE)
source("~/Dropbox/Teaching/BayesLearning/Hidden/Exam/20220525/BLExam220525.R", echo=TRUE)
source("~/Dropbox/Teaching/BayesLearning/Hidden/Exam/20220525/BLExam220525.R", echo=TRUE)
mu_0 = rep(0,5)
Omega_0 = 1*diag(5)
v_0 = 5
sigma2_0 = 5
nIter = 10000
res = BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter)
par(mfrow = c(2,3))
hist(sqrt(res$sigma2Sample), 50, main = "sigma")
hist(res$betaSample[,1], 50, main = expression(beta[0]))
hist(res$betaSample[,2], 50, main = expression(beta[1]))
hist(res$betaSample[,3], 50, main = expression(beta[2]))
hist(res$betaSample[,4], 50, main = expression(beta[3]))
hist(res$betaSample[,5], 50, main = expression(beta[4]))
# Problem 4b)
message(paste("The 95% equal tail interval for the coefficient on weight is "))
quantile(res$betaSample[,2], c(0.025,0.975))
# The interval includes zero, so weight is not a "signficant" variable in this
# Problem 4c)
# Problem 4c)
# Use the draws from a and produce a prediction for each draw to build a histogram
# Problem 4c)
# Use the draws from a and produce a prediction for each draw to build a histogram
# of the predictive distribution.
# The interval includes zero, so weight is not a "significant" variable in this
# Problem 4c)
# Problem 4c)
# Use the draws from a and produce a prediction for each draw to build a histogram
# Problem 4c)
# Use the draws from a and produce a prediction for each draw to build a histogram
# of the predictive distribution.
# Problem 4c)
# Use the draws from a and produce a prediction for each draw to build a histogram
# of the predictive distribution.
# Problem 4c)
# Use the draws from a and produce a prediction for each draw to build a histogram
# of the predictive distribution.
xStar = c(1,6,0,0,1)
i = 1
xStar%*%betaSample[i,] + rnorm(1, sd = sqrt(res$sigma2Sample[i]))
xStar%*%res$betaSample[i,] + rnorm(1, sd = sqrt(res$sigma2Sample[i]))
yPred = rep(NA,nIter)
for (i in 1:nIter){
yPred[i] = xStar%*%res$betaSample[i,] + rnorm(1, sd = sqrt(res$sigma2Sample[i]))
}
hist(yPred, 50)
hist(yPred, 40)
library(RandomFields)
#set.seed(71) # Set the seed for reproducibility
n = c(100,100)
#set.seed(71) # Set the seed for reproducibility
n = c(40,40)
Deltas = c(1,0.5)
x1 = seq(0,(n[1]-1)*Deltas[1], by = Deltas[1])
x2 = seq(0,(n[2]-1)*Deltas[2], by = Deltas[2])
# Simulate from the Gaussian random field over the parcels and compute prior inclusion probabilities
sigma = 10          # Prior variance
ell = 20            # Kernel length scale
nu = 30/2            # Smoothness
nugget = 1          # Nugget
#model = RMexp(var=sigma^2, scale=ell) + RMnugget(var=nugget)                     # Exponential kernel with nugget
model = RMmatern(nu = nu, var=sigma^2, scale=ell) + RMnugget(var=nugget)          # Matern(nu) kernel with nugget
#model = RMgauss(var=sigma^2, scale=ell) + RMnugget(var=nugget)                    # Gaussian kernel with nugget (Not quite squared exp?)
S = RandomFields::RFsimulate(model, x1, x2)  # Simulate a GRF over the parcel raster
RFoptions(install="no")
#set.seed(71) # Set the seed for reproducibility
n = c(40,40)
Deltas = c(1,0.5)
x1 = seq(0,(n[1]-1)*Deltas[1], by = Deltas[1])
x2 = seq(0,(n[2]-1)*Deltas[2], by = Deltas[2])
# Simulate from the Gaussian random field over the parcels and compute prior inclusion probabilities
sigma = 10          # Prior variance
ell = 20            # Kernel length scale
nu = 30/2            # Smoothness
nugget = 1          # Nugget
#model = RMexp(var=sigma^2, scale=ell) + RMnugget(var=nugget)                     # Exponential kernel with nugget
model = RMmatern(nu = nu, var=sigma^2, scale=ell) + RMnugget(var=nugget)          # Matern(nu) kernel with nugget
#model = RMgauss(var=sigma^2, scale=ell) + RMnugget(var=nugget)                    # Gaussian kernel with nugget (Not quite squared exp?)
S = RandomFields::RFsimulate(model, x1, x2)  # Simulate a GRF over the parcel raster
library(RandomFields)
RFoptions(install="no")
#set.seed(71) # Set the seed for reproducibility
n = c(40,40)
Deltas = c(1,0.5)
x1 = seq(0,(n[1]-1)*Deltas[1], by = Deltas[1])
x2 = seq(0,(n[2]-1)*Deltas[2], by = Deltas[2])
# Simulate from the Gaussian random field over the parcels and compute prior inclusion probabilities
sigma = 10          # Prior variance
ell = 20            # Kernel length scale
nu = 30/2            # Smoothness
nugget = 1          # Nugget
#model = RMexp(var=sigma^2, scale=ell) + RMnugget(var=nugget)                     # Exponential kernel with nugget
model = RMmatern(nu = nu, var=sigma^2, scale=ell) + RMnugget(var=nugget)          # Matern(nu) kernel with nugget
#model = RMgauss(var=sigma^2, scale=ell) + RMnugget(var=nugget)                    # Gaussian kernel with nugget (Not quite squared exp?)
S = RandomFields::RFsimulate(model, x1, x2)  # Simulate a GRF over the parcel raster
library(RandomFields)
#set.seed(71) # Set the seed for reproducibility
n = c(40,40)
Deltas = c(1,0.5)
x1 = seq(0,(n[1]-1)*Deltas[1], by = Deltas[1])
x2 = seq(0,(n[2]-1)*Deltas[2], by = Deltas[2])
# Simulate from the Gaussian random field over the parcels and compute prior inclusion probabilities
sigma = 10          # Prior variance
ell = 20            # Kernel length scale
nu = 30/2            # Smoothness
nugget = 1          # Nugget
#model = RMexp(var=sigma^2, scale=ell) + RMnugget(var=nugget)                     # Exponential kernel with nugget
model = RMmatern(nu = nu, var=sigma^2, scale=ell) + RMnugget(var=nugget)          # Matern(nu) kernel with nugget
#model = RMgauss(var=sigma^2, scale=ell) + RMnugget(var=nugget)                    # Gaussian kernel with nugget (Not quite squared exp?)
S = RandomFields::RFsimulate(model, x1, x2)  # Simulate a GRF over the parcel raster
RFoptions(install="yes")
library(RandomFields)
#set.seed(71) # Set the seed for reproducibility
n = c(70,70)
Deltas = c(1,0.5)
x1 = seq(0,(n[1]-1)*Deltas[1], by = Deltas[1])
x2 = seq(0,(n[2]-1)*Deltas[2], by = Deltas[2])
# Simulate from the Gaussian random field over the parcels and compute prior inclusion probabilities
sigma = 10          # Prior variance
ell = 20            # Kernel length scale
nu = 30/2            # Smoothness
nugget = 1          # Nugget
#model = RMexp(var=sigma^2, scale=ell) + RMnugget(var=nugget)                     # Exponential kernel with nugget
model = RMmatern(nu = nu, var=sigma^2, scale=ell) + RMnugget(var=nugget)          # Matern(nu) kernel with nugget
#model = RMgauss(var=sigma^2, scale=ell) + RMnugget(var=nugget)                    # Gaussian kernel with nugget (Not quite squared exp?)
S = RandomFields::RFsimulate(model, x1, x2)  # Simulate a GRF over the parcel raster
image(x1,x2,Y) # Plots the realized field
x1 = seq(0,(n[1]-1)*Deltas[1], by = Deltas[1])
x2 = seq(0,(n[2]-1)*Deltas[2], by = Deltas[2])
# Simulate from the Gaussian random field over the parcels and compute prior inclusion probabilities
sigma = 10          # Prior variance
ell = 20            # Kernel length scale
nu = 30/2            # Smoothness
nugget = 1          # Nugget
#model = RMexp(var=sigma^2, scale=ell) + RMnugget(var=nugget)                     # Exponential kernel with nugget
model = RMmatern(nu = nu, var=sigma^2, scale=ell) + RMnugget(var=nugget)          # Matern(nu) kernel with nugget
#model = RMgauss(var=sigma^2, scale=ell) + RMnugget(var=nugget)                    # Gaussian kernel with nugget (Not quite squared exp?)
S = RandomFields::RFsimulate(model, x1, x2)  # Simulate a GRF over the parcel raster
